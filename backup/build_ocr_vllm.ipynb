{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCR with vision-language models with vLLM\n",
    "\n",
    "This notebook provides a step-by-step guide to building an OCR system\n",
    "using vision-language models with vLLM and Gradio for inference.\n",
    " \n",
    "Optical Character Recognition (OCR) is essential for extracting text from images, scanned documents, and handwritten content.\n",
    "Vision-language models (VLMs) enhance OCR by leveraging transformer-based architectures, enabling context-aware text recognition.\n",
    " \n",
    "This tutorial explores how to use models like LLaVA, BLIP-2, and Qwen-VL for OCR. It covers the following topics:\n",
    "\n",
    "[Installing dependencies](#install-deps)  \n",
    "[Building an OCR with vLLM](#cli-ocr)  \n",
    "[Transforming the OCR to a GUI-enabled system with multiple model choices](#gradio-gui)\n",
    "\n",
    "\n",
    "The tutorial uses vLLM for large language model (LLM) inference. vLLM optimizes text generation workloads by effectively batching requests and utilizing GPU resources, offering high throughput and low latency, which is perfect for chatbots.\n",
    "\n",
    "![OCR Example](./assets/ocr.gif)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "### Hugging Face API access\n",
    "\n",
    "* Obtain an API token from [Hugging Face](https://huggingface.co) for downloading models.\n",
    "* Ensure the Hugging Face API token has the necessary permissions and approval to access the [Meta Llama checkpoints](https://huggingface.co/meta-llama/Llama-3.1-8B)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"install-deps\"></a>\n",
    "## 1. Installing dependencies\n",
    "\n",
    "Install the libraries needed for this tutorial. Run the following commands inside the Jupyter notebook running within the Docker container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install gradio requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide your Hugging Face token\n",
    "\n",
    "You'll require a Hugging Face API token to access Llama-3.1-8B. Generate your token at [Hugging Face Tokens](https://huggingface.co/settings/tokens) and request access for [Llama-3.2-11B-Vision-Instruct](https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct). Tokens typically start with \"hf_\".\n",
    "\n",
    "Run the following interactive block in your Jupyter notebook to set up the token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login, HfApi\n",
    "\n",
    "# Prompt the user to log in\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Verify that your token was accepted correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Validate the token\n",
    "try:\n",
    "    api = HfApi()\n",
    "    user_info = api.whoami()\n",
    "    print(f\"Token validated successfully! Logged in as: {user_info['name']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Token validation failed. Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cli-ocr\"></a>\n",
    "## 2. Building an OCR with vLLM\n",
    "\n",
    "First, define the `ImageInference` inference class. This class provides a constructor to initialize a model for inference. Additionally, it defines another function that runs inference on an image that you provide. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "from PIL import Image\n",
    "from vllm.multimodal.utils import fetch_image\n",
    "import sys\n",
    "\n",
    "current_model = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "\n",
    "class ImageInference:\n",
    "    def __init__(self, model_name=current_model):\n",
    "        # Initialize the model and tokenizer\n",
    "        self.llm = LLM(model=model_name, max_model_len=4096, max_num_seqs=16, enforce_eager=True)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    def generate_image_output(self, image: Image) -> str:\n",
    "        # Prepare the prompt and the image for the model\n",
    "        messages_og = [{'role': 'user', 'content': \"<|image|> What is in this image?\"}]\n",
    "        messages = [{\n",
    "            'role': 'user',\n",
    "            'content': (\n",
    "                \"Act as an OCR assistant. Analyze the provided <|image|> image and:\\n\"\n",
    "                \"1. Identify and transcribe all visible text in the image exactly as it appears.\\n\"\n",
    "                \"2. Preserve the original line breaks, spacing, and formatting from the image.\\n\"\n",
    "                \"3. Output only the transcribed text, line by line, without adding any commentary or explanations or special characters.\\n\"\n",
    "            )\n",
    "        }]\n",
    "        prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "        sampling_params = SamplingParams(max_tokens=512, temperature=0.7)\n",
    "\n",
    "        # Generate output from the model\n",
    "        outputs = self.llm.generate({\n",
    "            \"prompt\": prompt,\n",
    "            \"multi_modal_data\": {\"image\": image},\n",
    "        }, sampling_params=sampling_params)\n",
    "\n",
    "        # Extract and return generated text\n",
    "        generated_text = outputs[0].outputs[0].text if outputs else \"No output generated.\"\n",
    "        return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Testing the OCR system\n",
    "Download [this test image](https://github.com/ROCm/gpuaidev/tree/main/docs/notebooks/assets/together_we_advance_.png) by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/ROCm/gpuaidev/main/docs/notebooks/assets/together_we_advance_.png\"\n",
    "filename = \"together_we_advance_.png\"\n",
    "\n",
    "response = requests.get(url)\n",
    "with open(filename, \"wb\") as file:\n",
    "    file.write(response.content)\n",
    "\n",
    "print(\"Download complete:\", filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now it's time to test your OCR system. First read the image, then view it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "pil_image = Image.open(\"together_we_advance_.png\")\n",
    "pil_image = pil_image.convert(\"RGB\")  # Ensure the image is in RGB format\n",
    "pil_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, initialize an instance of the `ImageInference` class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the inference class\n",
    "inference = ImageInference()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now pass the image to the model for inference and print the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Generate output for the image\n",
    "output = inference.generate_image_output(pil_image)\n",
    "\n",
    "# Print the result\n",
    "print(\"Model Output:\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations. You've just built an OCR system. That's how simple it is to create an OCR-like system that transcribes the text in a given image. \n",
    "\n",
    "<a id=\"gradio-gui\"></a>\n",
    "## 3. Transforming the OCR to a GUI-enabled system with multiple model choices\n",
    "\n",
    "To provide a graphical interface for your chatbot, use [Gradio](https://www.gradio.app/) to create an interactive web-based UI.\n",
    "\n",
    "### Import Gradio and define a list of VLM models to access \n",
    "\n",
    "You're going to create a shortlist of handpicked models that can analyze images. The full list is available [here](https://docs.vllm.ai/en/latest/models/supported_models.html). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Define available models and their Hugging Face model IDs\n",
    "MODEL_OPTIONS = {\n",
    "    \"Llama-3.2-11B-Vision-Instruct\": \"meta-llama/Llama-3.2-11B-Vision-Instruct\",\n",
    "    \"BLIP-2 (2.7B)\": \"Salesforce/blip2-opt-2.7b\",\n",
    "    \"BLIP-2 (6.7B)\": \"Salesforce/blip2-opt-6.7b\",\n",
    "    \"Fuyu (8B)\": \"adept/fuyu-8b\",\n",
    "    \"GLM-4V (9B)\": \"THUDM/glm-4v-9b\",\n",
    "    \"H2OVL Mississippi (2B)\": \"h2oai/h2ovl-mississippi-2b\",\n",
    "    \"H2OVL Mississippi (800M)\": \"h2oai/h2ovl-mississippi-800m\",\n",
    "    \"InternVL2 (4B)\": \"OpenGVLab/InternVL2-4B\",\n",
    "    \"InternVL2 (2B)\": \"OpenGVLab/Mono-InternVL-2B\",\n",
    "    \"LLaVA 1.5 (7B)\": \"llava-hf/llava-1.5-7b-hf\",\n",
    "    \"LLaVA 1.5 (13B)\": \"llava-hf/llava-1.5-13b-hf\",\n",
    "    \"MiniCPM-V (2_5)\": \"openbmb/MiniCPM-Llama3-V-2_5\",\n",
    "    \"MiniCPM-V (2)\": \"openbmb/MiniCPM-V-2\",\n",
    "    \"Molmo (7B)\": \"allenai/Molmo-7B-D-0924\",\n",
    "    \"PaliGemma (3B PT)\": \"google/paligemma-3b-pt-224\",\n",
    "    \"PaliGemma (3B Mix)\": \"google/paligemma-3b-mix-224\",\n",
    "    \"Phi-3 Vision (128K)\": \"microsoft/Phi-3-vision-128k-instruct\",\n",
    "    \"Phi-3.5 Vision\": \"microsoft/Phi-3.5-vision-instruct\",\n",
    "    \"Pixtral (12B)\": \"mistralai/Pixtral-12B-2409\",\n",
    "    \"Qwen-VL\": \"Qwen/Qwen-VL\",\n",
    "    \"Qwen-VL-Chat\": \"Qwen/Qwen-VL-Chat\",\n",
    "    \"Qwen2-VL (2B)\": \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    \"Qwen2-VL (7B)\": \"Qwen/Qwen2-VL-7B-Instruct\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Gradio interface\n",
    "\n",
    "Next, define two simple helper functions for switching models and running inference using your previously defined class, followed by Gradio blocks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def switch_model(model_name):\n",
    "    global inference\n",
    "    try:\n",
    "        inference = ImageInference(model_name=model_name)\n",
    "        return f\"Switched to model: {model_name}\"\n",
    "    except Exception as e:\n",
    "        return f\"Failed to switch model: {str(e)}\"\n",
    "\n",
    "def analyze_image(image):\n",
    "    try:\n",
    "        pil_image = image.convert(\"RGB\")\n",
    "        result = inference.generate_image_output(pil_image)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return f\"Error processing the image: {str(e)}\"\n",
    "\n",
    "with gr.Blocks() as interface:\n",
    "    gr.Markdown(\"## Multimodal OCR & Image Analysis\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        model_dropdown = gr.Dropdown(\n",
    "            choices=list(MODEL_OPTIONS.keys()),\n",
    "            value=\"Llama-3.2-11B-Vision-Instruct\",\n",
    "            label=\"Select Model\",\n",
    "        )\n",
    "        switch_button = gr.Button(\"Switch Model\")\n",
    "\n",
    "    model_status = gr.Textbox(value=f\"Current Model: {current_model}\", label=\"Model Status\")\n",
    "\n",
    "    image_input = gr.Image(type=\"pil\", label=\"Upload Image\")\n",
    "    analyze_button = gr.Button(\"Analyze Image\")\n",
    "    # output_box = gr.Textbox(label=\"Model Output\", lines=10)\n",
    "    output_box = gr.Markdown(label=\"Model Output\")\n",
    "\n",
    "    switch_button.click(\n",
    "        fn=lambda selected_model: (switch_model(MODEL_OPTIONS[selected_model]), f\"Current Model: {MODEL_OPTIONS[selected_model]}\"),\n",
    "        inputs=[model_dropdown],\n",
    "        outputs=[model_status, model_status],\n",
    "    )\n",
    "    analyze_button.click(fn=analyze_image, inputs=image_input, outputs=output_box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Run the Gradio application\n",
    "\n",
    "Execute the code block below to launch the GUI. The interface displays in your browser, letting you interact with the OCR system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "interface.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "In this tutorial, you accomplished the following tasks:\n",
    "\n",
    "* Built an OCR class using vLLM.\n",
    "* Extended the functionality by adding a GUI and a selection of multiple different models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Happy coding! If you encounter issues or have questions, don’t hesitate to ask or raise an issue on our [Github page](https://github.com/ROCm/gpuaidev)!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
