{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying DeepSeek-LLM-7B-Chat using vLLM\n",
    "\n",
    "vLLM is an open-source library designed to deliver high throughput and low latency for large language model (LLM) inference. It optimizes text generation workloads by efficiently batching requests and making full use of GPU resources, empowering developers to manage complex tasks like code generation and large-scale conversational AI.\n",
    "\n",
    "This tutorial guides you through setting up and running vLLM on AMD Instinctâ„¢ GPUs using the ROCm software stack. Learn how to configure your environment, containerize your workflow, and send test queries to the vLLM-supported inference server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying the LLM using vLLM\n",
    "\n",
    "Start deploying the LLM (meta-llama/Llama-3.1-8B-Instruct) using vLLM in the Jupyter notebook:\n",
    "\n",
    "### Start the vLLM server \n",
    "\n",
    "Open a new tab in this Jypyter server, click on the terminal icon to open a new terminal, then copy the following command to launch the vLLM server:\n",
    "\n",
    "```bash\n",
    "HIP_VISIBLE_DEVICES=0 vllm serve meta-llama/Meta-Llama-3.1-8B-Instruct \\\n",
    "        --gpu-memory-utilization 0.9 \\\n",
    "        --swap-space 16 \\\n",
    "        --disable-log-requests \\\n",
    "        --dtype float16 \\\n",
    "        --max-model-len 131072 \\\n",
    "        --tensor-parallel-size 1 \\\n",
    "        --host 0.0.0.0 \\\n",
    "        --port 3000 \\\n",
    "        --num-scheduler-steps 10 \\\n",
    "        --max-num-seqs 128 \\\n",
    "        --max-num-batched-tokens 131072 \\\n",
    "        --max-model-len 131072 \\\n",
    "        --distributed-executor-backend \"mp\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After successfully connecting, it displays `INFO:     Application startup complete.`.\n",
    "\n",
    "**Note**: In a multi-GPU environment, the setting `HIP_VISIBLE_DEVICES=x` is recommended to deploy the LLM on your preferred GPU.\n",
    "\n",
    "### Start the client\n",
    "\n",
    "After successfully running the server, as described above, run the following code to start your client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'chatcmpl-70f5e43b166347388234b0226355b030', 'object': 'chat.completion', 'created': 1749483992, 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'reasoning_content': None, 'content': 'Artificial Intelligence (AI) is a field of computer science that focuses on creating intelligent machines that can perform tasks that typically require human intelligence, such as learning, problem-solving, decision-making, and perception. These machines are designed to simulate human-like intelligence, enabling them to interact with their environment, understand and interpret data, and make decisions based on that information. AI systems use algorithms, data, and computational power to analyze and process information, allowing them to learn from experience and improve their performance over time.', 'tool_calls': []}, 'logprobs': None, 'finish_reason': 'stop', 'stop_reason': None}], 'usage': {'prompt_tokens': 62, 'total_tokens': 164, 'completion_tokens': 102, 'prompt_tokens_details': None}, 'prompt_logprobs': None, 'kv_transfer_params': None}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://localhost:3000/v1/chat/completions\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "data = {\n",
    "    \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are an expert in the field of AI. Make sure to provide an explanation in few sentences.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the concept of AI.\"\n",
    "        }\n",
    "    ],\n",
    "    \"stream\": False,\n",
    "    \"max_tokens\": 128\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Remember to match the Docker `--port` **3000** and the port indicated in the URL, for instance, http://localhost:**3000**. If the port is already used by another application, you can modify the number. \n",
    "\n",
    "If the connection is successful, the output will be:\n",
    "\n",
    "``` bash\n",
    "{\"id\":\"chat-xx\",\"object\":\"chat.completion\",\"created\":1736494622,\"model\":\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"Artificial Intelligence (AI) is a field of computer science ...}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
